#!/usr/bin/env python

import os
import sys
import argparse
import shutil
import subprocess
from os.path import basename 
from collections import defaultdict
from Bio.UniProt import GOA

import ArgParser_Benchmark as ap
import Config
import CreateBenchmark as cb
import FindDataset as fd
import FormatChecker as fc
import GOAParser_cafa as gc
import PaperTermFrequency as ptf
import verifyBenchmark as vb

'''
   The Benchmark verification program verifies the benchmarks created by 
   the Benchmark creation software.

   The program takes at least two inputs: input file 1: a tab delimited 
   annotation file at time t1, input file 2: a tab delimited annotation file 
   at time t2. Then the program finds the latest version of the benchmark 
   files created by Bechmark Creation program for this specific set of 
   input files and verifies the correctness of these benchmark files.    
 
   Verify program can also take a third input which is one of the benchmark 
   files created by Benchmark Creation program. When this third input file 
   is given, it extracts the version of the benchmark files from this input, 
   and verifies all the benchmark files for that specific version.  
 
   The program creates THREE intermediate files from the input file 1 and input 
   file 2 and these intermediate files are removed after the verfication is 
   completed.
   
   Complete usage directions of this program can obtained through the 
   following command: 

   python Verify --help
'''

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

config_filename = '.cafarc' # Default configuration file name

class Verify:
    def __init__(self):
        self.parsed_dict = ap.parse_args('verify')
                        # Obtain user supplied argument values in a dictionary 
        self.ConfigParam = Config.read_config(config_filename) # Collect config entries
        t1 = self.parsed_dict['t1'] # Retreive file name at time t1
        t2 = self.parsed_dict['t2'] # Retreive file name at time t2

        self.work_dir = (self.ConfigParam['workdir']).rstrip('/')
                                   # Retreive work directory name
        if not os.path.exists(self.work_dir):
            print ('Work space not found. Check ' + config_filename + ' file for correct assignment of work directory.')
            sys.exit(1)
#            os.makedirs(self.work_dir)

                # Create work direcoty, if it does not exist
        self.t1_input_file = fd.find_dataset(t1, self.work_dir,
                            self.ConfigParam) # Locate t1 file
        self.t2_input_file = fd.find_dataset(t2, self.work_dir,
                            self.ConfigParam) # Locate t2 file

        # Names for SIX benchmark files: bpo, cco, and mfo for LK and NK benchmark types:
        t3_basename = basename(self.parsed_dict['t3']) # Retreive base name of the benchmark file
        if t3_basename == '':
            t3_LK_bpo = self.work_dir + '/' + self.create_benchmarkFilename('LK_bpo')
            t3_basename = basename(t3_LK_bpo)
            benchmarkVersion = t3_basename.strip().split('.')[-1]
        else: 
            benchmarkVersion = t3_basename.strip().split('.')[-1]
        self.benchmark_LK_bpo = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                              '.benchmark_LK_bpo.' + benchmarkVersion
        self.benchmark_LK_cco = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                            '.benchmark_LK_cco.' + benchmarkVersion
        self.benchmark_LK_mfo = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                            '.benchmark_LK_mfo.' + benchmarkVersion
        self.benchmark_NK_bpo = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                            '.benchmark_NK_bpo.' + benchmarkVersion
        self.benchmark_NK_cco = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                            '.benchmark_NK_cco.' + benchmarkVersion
        self.benchmark_NK_mfo = self.work_dir + '/' + self.parsed_dict['t2'] + \
                            '-' + ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
                            '.benchmark_NK_mfo.' + benchmarkVersion

        # Names for THREE files to store non-EXP and EXP type entries:
        # These files will be deleted once the calculation is done
        self.t1_iea_name = self.t1_input_file + '.iea'
                # File name for entries in t1 file with non-EXP evidence codes
        self.t1_exp_name = self.t1_input_file + '.exp'
                # File name for entries in t1 file with EXP evidence codes
        self.t2_exp_name = self.t2_input_file + '.exp'
                # File name for entries in t2 file with EXP evidence codes

        # Name for GO ID frequency per pubmed id for t2 file:
        # This file will be deleted once the calculations are done
        self.t2_ptf_file =  self.t2_input_file + '_with_annotations_per_paper.txt'
        
    def create_benchmarkFilename(self, ontType):
        # Creates a benchmark filename based on input file names at times 
        # t1 and t2
        ob = basename(self.parsed_dict['t2']) + '-' + \
            ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
            '.benchmark' + '_' + ontType
        index = 1
        while os.path.exists(self.work_dir + '/' + ob + '.' + str(index)):
            index = index + 1
        benchmark_filename = ob + '.' + str(index-1)
        return benchmark_filename

    def create_iterator(self, infile):
        # Returns an iterator object for an input uniprot-goa file along with a 
        # list of all field names contained in the uniprot-goa file

        infile_handle = open(infile, 'r')
        iter_handle = GOA.gafiterator(infile_handle)
        for ingen in iter_handle:
            if len(ingen) == 17:
                GAFFIELDS = GOA.GAF20FIELDS
                break
            else:
                GAFFIELDS = GOA.GAF10FIELDS
                break
        infile_handle = open(infile, 'r')
        iter_handle = GOA.gafiterator(infile_handle)
        return iter_handle, GAFFIELDS

    def create_intermediate_files(self):
        # Create paper-term freq file for t2 file:
        ann_conf = ptf.paper_term_freq( open(self.t2_input_file,'r'),
                                        open(self.t2_ptf_file,'w'),
                                        self.parsed_dict)
        # Create an iterator object for filtering t2 file:
        iter_handle, GAFFIELDS = self.create_iterator(self.t2_input_file)

        # Create tax_id_name_mapping for filtering t2 file:
        tax_id_name_mapping = gc.parse_tax_file(self.ConfigParam['tax_file'])

        # Create t2_exp_name file:
        # Filter t2 file for all proteins with EXP evidence:
        print 'Parsing t2 file: ' + basename(self.t2_input_file) + ' ...'
        t2_exp_handle = open(self.t2_exp_name, 'w')
        for ingen in iter_handle: # Iterate through t2 file entries
            retval = gc.record_has_forBenchmark(ingen,
                                                ann_conf,
                                                self.parsed_dict,
                                                tax_id_name_mapping,
                                                self.ConfigParam['exp_eec'],
                                                GAFFIELDS)
            if retval: # If retval is TRUE, write out the record to t2.exp file
                GOA.writerec(ingen, t2_exp_handle, GAFFIELDS) # Create t2.exp file
        t2_exp_handle.close()

        if os.stat(self.t2_exp_name).st_size == 0: # If t2.exp is empty, program quits
            print 'Your benchmark set will be empty with the parameters provided. Quiting ...'
            sys.exit(1)
       
        # Create t1.iea_name and t1.exp_name files: 
        # Filter t1 file to create t1.iea and t1.exp files
        iter_handle, GAFFIELDS = self.create_iterator(self.t1_input_file)
        print 'Parsing t1 file: ' + basename(self.t1_input_file) + ' ...' 
        gc.t1_filter(iter_handle, self.t1_iea_name, self.t1_exp_name, 
                    self.t2_exp_name, GAFFIELDS, self.ConfigParam['exp_eec'])
        return None 

    def delete_intermediate_files(self):
        print 'Cleaning working directory ...'
        # Delete t1.iea, t1.exp, and t2.exp files:
        os.remove(self.t1_iea_name)
        os.remove(self.t1_exp_name)
        os.remove(self.t2_exp_name)
        # Delete paper term frequency file for t2 file:
        os.remove(self.t2_ptf_file)
        # Delete any empty files from the workspace (subdirectories included):
        for root, dirs, files in os.walk(self.work_dir):
            for fname in files:
                if os.path.getsize(root + '/' + fname) == 0:
                    os.remove(root + '/' + fname)
            break
        return None

    def print_prolog(self):
        print "*************************************************"
        print "Welcome to Benchmark Verification Tool !!!!!"
        print "*************************************************\n"
        print 'Following is a list of user supplied inputs:\n'
        for arg in self.parsed_dict:
            print arg + ': ' + str(self.parsed_dict[arg])
        print '*********************************************\n'
        return None

    def print_epilog(self, LKcount, NKcount):
        if LKcount + NKcount == 0:
            print bcolors.WARNING + 'No valid benchmark file found for verification' + \
                bcolors.ENDC
        else: 
            print bcolors.OKGREEN + 'Verification successful for the following benchmark files:' + \
                bcolors.ENDC
            if os.path.exists(self.benchmark_LK_bpo) and os.stat(self.benchmark_LK_bpo).st_size != 0:
                print basename(self.benchmark_LK_bpo)
            if os.path.exists(self.benchmark_LK_cco) and os.stat(self.benchmark_LK_cco).st_size != 0:
                print basename(self.benchmark_LK_cco)
            if os.path.exists(self.benchmark_LK_mfo) and os.stat(self.benchmark_LK_mfo).st_size != 0:
                print basename(self.benchmark_LK_mfo)
            if os.path.exists(self.benchmark_NK_bpo) and os.stat(self.benchmark_NK_bpo).st_size != 0:
                print basename(self.benchmark_NK_bpo)
            if os.path.exists(self.benchmark_NK_cco) and os.stat(self.benchmark_NK_cco).st_size != 0:
                print basename(self.benchmark_NK_cco)
            if os.path.exists(self.benchmark_NK_mfo) and os.stat(self.benchmark_NK_mfo).st_size != 0:
                print basename(self.benchmark_NK_mfo)
        print bcolors.OKGREEN + 'Thank you for using Benchmark Verification Tool' + \
            bcolors.ENDC
        return None

    def process_data(self): # Process user data and create benchmark sets
        self.print_prolog() # Print the welcome message and argument list
        fc.check_gaf_format(self.t1_input_file) # File format check for t1 file
        fc.check_gaf_format(self.t2_input_file) # File format check for t2 file
        self.create_intermediate_files() # Create necessary intermediate files
        # Verifying the benchmarks
        print 'Verifying benchmark sets ...'
        fc.check_benchmark_format(self.benchmark_LK_bpo)
        fc.check_benchmark_format(self.benchmark_LK_cco)
        fc.check_benchmark_format(self.benchmark_LK_mfo)
        LKcount = vb.verify_LK_benchmark(self.t1_iea_name, self.t1_exp_name, 
            self.t2_exp_name, self.benchmark_LK_bpo, self.benchmark_LK_cco, 
            self.benchmark_LK_mfo)
        fc.check_benchmark_format(self.benchmark_NK_bpo)
        fc.check_benchmark_format(self.benchmark_NK_cco)
        fc.check_benchmark_format(self.benchmark_NK_mfo)
        NKcount = vb.verify_NK_benchmark(self.t1_iea_name, self.t1_exp_name, 
            self.t2_exp_name, self.benchmark_NK_bpo, self.benchmark_NK_cco, 
            self.benchmark_NK_mfo)
        self.delete_intermediate_files() # Delete intermediate files
        self.print_epilog(LKcount, NKcount) # Print summary of running this program
        return None

if __name__ == '__main__': 
    vm = Verify() # Create a Verify instance 
    vm.process_data() # Process data and create benchmark sets
