#!/usr/bin/env python

'''
    The Benchmark Verification Tool verifies the benchmarks created by
    Benchmark Creation Tool.

    The program takes at least three inputs: input file 1: a tab delimited
    annotation file at time t1, input file 2: a tab delimited annotation file
    at time t2, and 3. one of the benchmark file names created by the
    Benchmark Creation Tool using the the first and the second input files
    given here. Simplest way to run this program is:

    This tool will verify the correctness of all the benchmark files of the
    specific version as the third input that are available in the workspace.

    This tool creates some intermediate files and removes them after the
    verification is completed.
   
    Complete usage directions of this program can be obtained through the
    following command:

    python Verify --help
'''

import os
import sys
import argparse
import shutil
import subprocess
from os.path import basename 
from collections import defaultdict
from Bio.UniProt import GOA

import ArgParser_Benchmark as ap
import Config
import CreateBenchmark as cb
import LocateDataset as ld
import FormatChecker as fc
import GOAParser_cafa as gc
import PaperTermFrequency as ptf
import verifyBenchmark as vb

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

config_filename = '.cafarc' # Default configuration file name

class Verify:
    def __init__(self):
        self.parsed_dict = ap.parse_args('verify')
                        # Obtain user supplied argument values in a dictionary 
        self.ConfigParam = Config.read_config(config_filename) # Collect config file entries
        t1 = self.parsed_dict['t1'] # Retreive file name at time t1
        t2 = self.parsed_dict['t2'] # Retreive file name at time t2
        t3 = self.parsed_dict['t3'] # Retreive file name at time t2

        self.work_dir = (self.ConfigParam['workdir']).rstrip('/')
                                   # Retreive work directory name
        if not os.path.exists(self.work_dir):
            print ('Work space not found. Check ' + config_filename + \
                ' file for correct assignment of work directory.')
            sys.exit(1)
        self.t1_input_file = ld.locate_GOAfile(t1, self.work_dir)

        self.t2_input_file = ld.locate_GOAfile(t2, self.work_dir)

        # Names for SIX benchmark files: bpo, cco, and mfo for LK and NK benchmark types:
        benchmarkVersion = basename(t3).strip().split('.')[-1]
        self.benchmark_LK_bpo = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_LK_bpo.' + benchmarkVersion
        self.benchmark_LK_cco = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_LK_cco.' + benchmarkVersion
        self.benchmark_LK_mfo = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_LK_mfo.' + benchmarkVersion
        self.benchmark_NK_bpo = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_NK_bpo.' + benchmarkVersion
        self.benchmark_NK_cco = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_NK_cco.' + benchmarkVersion
        self.benchmark_NK_mfo = self.t2_input_file + '-' + \
                                (basename(t1).split('.'))[-1] + \
                                '.benchmark_NK_mfo.' + benchmarkVersion

        # Names for THREE files to store non-EXP and EXP type entries:
        # These files will be deleted once the calculation is done
        self.t1_iea_name = self.t1_input_file + '.iea'
                # File name for entries in t1 file with non-EXP evidence codes
        self.t1_exp_name = self.t1_input_file + '.exp'
                # File name for entries in t1 file with EXP evidence codes
        self.t2_exp_name = self.t2_input_file + '.exp'
                # File name for entries in t2 file with EXP evidence codes

        # Name for GO ID frequency per pubmed id for t2 file:
        # This file will be deleted once the calculations are done
        self.t2_ptf_file =  self.t2_input_file + '_with_annotations_per_paper.txt'
        
    def create_benchmarkFilename(self, ontType):
        # Creates a benchmark filename based on input file names at times 
        # t1 and t2
        ob = basename(self.parsed_dict['t2']) + '-' + \
            ((basename(self.parsed_dict['t1'])).split('.'))[-1] + \
            '.benchmark' + '_' + ontType
        index = 1
        while os.path.exists(self.work_dir + '/' + ob + '.' + str(index)):
            index = index + 1
        benchmark_filename = ob + '.' + str(index-1)
        return benchmark_filename

    def create_iterator(self, infile):
        # Returns an iterator object for an input uniprot-goa file along with a 
        # list of all field names contained in the uniprot-goa file

        infile_handle = open(infile, 'r')
        iter_handle = GOA.gafiterator(infile_handle)
        for ingen in iter_handle:
            if len(ingen) == 17:
                GAFFIELDS = GOA.GAF20FIELDS
                break
            else:
                GAFFIELDS = GOA.GAF10FIELDS
                break
        infile_handle = open(infile, 'r')
        iter_handle = GOA.gafiterator(infile_handle)
        return iter_handle, GAFFIELDS

    def locate_benchmark_files(self):
        allFound = True
        if(ld.locate_benchmark_file(self.benchmark_LK_bpo, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_LK_bpo = ''
        if(ld.locate_benchmark_file(self.benchmark_LK_cco, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_LK_cco = ''

        if(ld.locate_benchmark_file(self.benchmark_LK_mfo, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_LK_mfo = ''

        if(ld.locate_benchmark_file(self.benchmark_NK_bpo, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_NK_bpo = ''
        if(ld.locate_benchmark_file(self.benchmark_NK_cco, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_NK_cco = ''

        if(ld.locate_benchmark_file(self.benchmark_NK_mfo, self.work_dir)): 
            pass
        else: 
            allFound = False
            self.benchmark_NK_mfo = ''
        return allFound

    def create_intermediate_files(self):
        # Create paper-term freq file for t2 file:
        ann_conf = ptf.paper_term_freq( open(self.t2_input_file,'r'),
                                        open(self.t2_ptf_file,'w'),
                                        self.parsed_dict)
        # Create an iterator object for filtering t2 file:
        iter_handle, GAFFIELDS = self.create_iterator(self.t2_input_file)

        # Create tax_id_name_mapping for filtering t2 file:
        tax_id_name_mapping = gc.parse_tax_file(self.ConfigParam['tax_file'])

        # Create t2_exp_name file:
        # Filter t2 file for all proteins with EXP evidence:
        print 'Parsing t2 file: ' + basename(self.t2_input_file) + ' ...'
        t2_exp_handle = open(self.t2_exp_name, 'w')
        for ingen in iter_handle: # Iterate through t2 file entries
            retval = gc.record_has_forBenchmark(ingen,
                                                ann_conf,
                                                self.parsed_dict,
                                                tax_id_name_mapping,
                                                self.ConfigParam['exp_eec'],
                                                GAFFIELDS)
            if retval: # If retval is TRUE, write out the record to t2.exp file
                GOA.writerec(ingen, t2_exp_handle, GAFFIELDS) # Create t2.exp file
        t2_exp_handle.close()

        if os.stat(self.t2_exp_name).st_size == 0: # If t2.exp is empty, program quits
            print 'Your benchmark set will be empty with the parameters provided. Quiting ...'
            sys.exit(1)
       
        # Create t1.iea_name and t1.exp_name files: 
        # Filter t1 file to create t1.iea and t1.exp files
        iter_handle, GAFFIELDS = self.create_iterator(self.t1_input_file)
        print 'Parsing t1 file: ' + basename(self.t1_input_file) + ' ...' 
        gc.t1_filter(iter_handle, self.t1_iea_name, self.t1_exp_name, 
                    self.t2_exp_name, GAFFIELDS, self.ConfigParam['exp_eec'])
        return None 

    def delete_intermediate_files(self):
        print 'Cleaning working directory ...'
        # Delete t1.iea, t1.exp, and t2.exp files:
        os.remove(self.t1_iea_name)
        os.remove(self.t1_exp_name)
        os.remove(self.t2_exp_name)
        # Delete paper term frequency file for t2 file:
        os.remove(self.t2_ptf_file)
        # Delete any empty files from the workspace (subdirectories included):
        for root, dirs, files in os.walk(self.work_dir):
            for fname in files:
                if os.path.getsize(root + '/' + fname) == 0:
                    os.remove(root + '/' + fname)
            break
        return None

    def print_prolog(self):
        print "*************************************************"
        print "Welcome to Benchmark Verification Tool !!!!!"
        print "*************************************************\n"
        print 'Following is a list of user supplied inputs:\n'
        for arg in self.parsed_dict:
            print arg + ': ' + str(self.parsed_dict[arg])
        print '*********************************************\n'
        return None

    def print_epilog(self, LKcount, NKcount):
        if LKcount + NKcount == 0:
            print bcolors.WARNING + \
                  'No valid benchmark file found for verification' + \
                   bcolors.ENDC
        else: 
            print bcolors.OKGREEN + \
                  'Verification successful for the following benchmark files:' + \
                  bcolors.ENDC
            if os.path.exists(self.benchmark_LK_bpo) and \
               os.stat(self.benchmark_LK_bpo).st_size != 0:
                print basename(self.benchmark_LK_bpo)
            if os.path.exists(self.benchmark_LK_cco) and \
               os.stat(self.benchmark_LK_cco).st_size != 0:
                print basename(self.benchmark_LK_cco)
            if os.path.exists(self.benchmark_LK_mfo) and \
               os.stat(self.benchmark_LK_mfo).st_size != 0:
                print basename(self.benchmark_LK_mfo)
            if os.path.exists(self.benchmark_NK_bpo) and \
               os.stat(self.benchmark_NK_bpo).st_size != 0:
                print basename(self.benchmark_NK_bpo)
            if os.path.exists(self.benchmark_NK_cco) and \
               os.stat(self.benchmark_NK_cco).st_size != 0:
                print basename(self.benchmark_NK_cco)
            if os.path.exists(self.benchmark_NK_mfo) and \
               os.stat(self.benchmark_NK_mfo).st_size != 0:
                print basename(self.benchmark_NK_mfo)
        print bcolors.OKGREEN + \
              'Thank you for using Benchmark Verification Tool' + \
              bcolors.ENDC
        return None

    def process_data(self):
        """ 
        This method is the entry point of Benchmark Verification Tool. 
        It invokes different functions to print welcome message, 
        format checking for input files, create necessary 
        intermediate files, checks file format of the benchmark 
        sets, verify correctness of the entries in the benchmark 
        sets, delete intermediate files, and print summary of 
        running this program.  
        """ 

        # Print the welcome message and user argument list:
        self.print_prolog() 
       
        # Locate benchmark files:
        if (not self.locate_benchmark_files()): 
            print ('No benchmark files found. Program quitting ...') 
            sys.exit(1)

        # File format check for t1 file:
        fc.check_gaf_format(self.t1_input_file) 
        # File format check for t2 file:
        fc.check_gaf_format(self.t2_input_file) 

        # Create necessary intermediate files:
        self.create_intermediate_files() 
      
        # Locate the benchmark files: 
        self.locate_benchmark_files()

        # Check file format for LK-benchmark sets:
        if (not self.benchmark_LK_bpo): # if the file exists, check its format
            fc.check_benchmark_format(self.benchmark_LK_bpo)
        if (not self.benchmark_LK_cco):
            fc.check_benchmark_format(self.benchmark_LK_cco)
        if (not self.benchmark_LK_mfo):
            fc.check_benchmark_format(self.benchmark_LK_mfo)

        # Verifying the benchmarks:
        print 'Verifying benchmark sets ...'
        # Verify LK-benchmark sets:

        LKcount = vb.verify_LK_benchmark(open(self.t1_iea_name, 'r'),
                                         open(self.t1_exp_name, 'r'),
                                         open(self.t2_exp_name, 'r'),
                                         self.benchmark_LK_bpo,
                                         self.benchmark_LK_cco,
                                         self.benchmark_LK_mfo)
        # Check file format for NK-benchmark sets:
        if (not self.benchmark_NK_bpo): # if the file exists, check its format
            fc.check_benchmark_format(self.benchmark_NK_bpo)
        if (not self.benchmark_NK_cco):
            fc.check_benchmark_format(self.benchmark_NK_cco)
        if (not self.benchmark_NK_mfo):
            fc.check_benchmark_format(self.benchmark_NK_mfo)

        # Verify NK-benchmark sets:
        NKcount = vb.verify_NK_benchmark(open(self.t1_iea_name, 'r'),
                                         open(self.t1_exp_name, 'r'),
                                         open(self.t2_exp_name, 'r'),
                                         self.benchmark_NK_bpo,
                                         self.benchmark_NK_cco,
                                         self.benchmark_NK_mfo)
        # Delete intermediate files:
        self.delete_intermediate_files()

        # Print summary of running this program:
        self.print_epilog(LKcount, NKcount) 
        return None

if __name__ == '__main__':
    if (sys.argv[0] == 'Verify' or sys.argv[0] == './Verify') and len(sys.argv) == 1:
        print(__doc__)
        sys.exit(0)
    else: 
        vm = Verify() # Create an instance of Verify class 
        vm.process_data() # Process data and verify the benchmark sets
        sys.exit(0)
