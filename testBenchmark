#!/usr/bin/env python
'''
    The methods in this module are designed to test Benchmark Creation and 
    Verification twin toolset.    
    
    How to run this program: 

       python testBenchmark -I=testDataset.txt -O='output.txt'
  
    testDataset lists the pairs of UniProt-GOA annotatation file names 
        at time points t1 and t2. 
    output.txt will have all the messages coming from running 
        Benchmark and Verify programs.
'''

import os
import sys
import subprocess
from os.path import basename

import commands
import StringIO
import urllib
import urllib2
import gzip 
import zipfile

import ArgParser_testBenchmark as ap
import Config

config_filename = '.cafarc' # Default configuration file name

class testBenchmark: 
    def __init__(self):
        self.parsed_dict = ap.parse_args()
        self.ConfigParam = Config.read_config(config_filename)
        self.work_dir = (self.ConfigParam['workdir'].rstrip('/'))

        self.testData_filename = self.work_dir + '/' + basename(self.parsed_dict['input1'])
        self.output_filename = self.work_dir + '/' + basename(self.parsed_dict['output1'])

        self.goa_arc = 'ftp://ftp.ebi.ac.uk/pub/databases/GO/goa/old'
        self.bmVersion = 1 # The benchmark verion number that will be verified

        if not os.path.exists(self.work_dir):
            os.makedirs(self.work_dir)
                # Create work direcoty, if it does not exist

    def download_testDataset(self, testDataset_fh):
        for line in testDataset_fh:
            if (not os.path.isfile(self.work_dir + '/' + line.strip())):
                fname = line.strip() + '.gz'
                folder_name = line.strip().split('.')[1].split('_')[-1]
                      # Organism specific folder name
                url = self.goa_arc + '/' + \
                      line.strip().split('.')[1].split('_')[-1].upper() +'/'
                     # Organism specific URL
                self.download(url, fname)
        return None

    def download(self, url, fname):
        print('Downloading ' + fname + ' ... ')
        try:
            response = urllib2.urlopen(url + fname)
        except urllib2.HTTPError, err:
            return False
        except urllib2.URLError, err: 
            return False
        out_fh = open(self.work_dir + '/' + fname, 'w')
        out_fh.write(response.read())
        out_fh.close()
        os.system('gzip -d ' + self.work_dir + '/' + fname)
        return True

    def exec_twinToolset(self, input1, input2):
        # Create benchmark files:
        cmd_benchmark = 'python' + ' ' + 'Benchmark' + ' ' + '-I1=' +\
                        input1 + ' ' + '-I2=' + input2 + '>>' +\
                        self.output_filename
        subprocess.call(cmd_benchmark, shell=True) 

        # Verify the benchmark files that are just created:
        input3 = basename(input2) + '-' + (basename(input1).split('.'))[-1] + \
                 '.benchmark_LK_bpo.' + str(self.bmVersion)
        cmd_verify = 'python' + ' ' + 'Verify' + ' ' + '-I1=' + input1 + ' ' +\
                     '-I2=' + input2 + ' ' + '-I3=' + input3 + '>>' +\
                     self.output_filename 
        subprocess.call(cmd_verify, shell=True)
        return None

    def run_test(self, testDataset_fh):
        for line in testDataset_fh:
            input1 = self.work_dir + '/' + line.rstrip('\n')
            input2 = self.work_dir + '/' + (next(testDataset_fh)).strip('\n')
            self.exec_twinToolset(input1, input2)
        return None

    def process_test(self):
        self.download_testDataset(open(self.testData_filename, 'r'))
        subprocess.call('echo ' +  'This is the output from running ' +\
                        'testBenchmark program' + ' ' + \
                        '>' + self.output_filename, shell=True)
        self.run_test(open(self.testData_filename, 'r'))
        subprocess.call('echo ' +  'End of running testBenchmark ' +\
                        'program' + ' ' + '>>' + \
                         self.output_filename, shell=True)
        return None

if __name__ == '__main__':
    if len(sys.argv) == 1:
        print (sys.argv[0] + ' docstring:')
        print(__doc__)
        sys.exit(0)
    else:
        testObject = testBenchmark()
        testObject.process_test()
        sys.exit(0)
