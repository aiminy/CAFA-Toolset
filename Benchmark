#!/usr/bin/env python

import os
import sys
import argparse
import shutil
import subprocess
from Bio.UniProt import GOA
from collections import defaultdict
from os.path import basename 

import ArgParser_Benchmark as ap
import Config
import ConfigParser as cp
import CreateDataset as cd
import FilterBenchmark as fb
import FormatChecker as fc
import GOAParser_cafa as gc
import PaperTermFrequency as ptf

'''
   Benchmark program accepts 2 uniprot-goa gaf-format files as inputs. These 
   are available for download at ftp://ftp.ebi.ac.uk/pub/databases/GO/goa/

   There are a host of other parameters that could be provided to filter 
   results. The list of all parameters can be obtained through the following 
   command:

   python Benchmark --help

   Output for Benchmark mode are SIX benchmark files: THREE limited-knolwedge 
   (LK) files - one in each of the three ontolgies (MFO, BPO, and CCO) and 
   THREE no-knowledge (NK) files - one in each ontology.
'''

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def paper_term_freq(params, infile):
    # Given an input uniprot-goa file, this method computes the number of 
    # proteins annotated in every pubmed id mentioned in that file

    if params['Confidence'] == 'T':
        paper_threshold = params['Threshold']
        ann_conf_filter = True
    else:
        ann_conf_filter = False
        paper_threshold = 0
    if not os.path.exists(infile + '_with_annotations_per_paper.txt'):
        paper_annotation_freq  = infile + '_with_annotations_per_paper.txt'
        paper_conf_filter = True
        paper_ann_freq_handle = open(paper_annotation_freq, 'w')
    else:
        paper_conf_filter = False
    
    if ann_conf_filter or paper_conf_filter : 
        [ann_conf, paper_conf] = ptf.count(infile, params['Evidence'], 
                                                          ann_conf_filter, paper_conf_filter)
        if len(paper_conf) > 0:
            for i in paper_conf:
                print >> paper_ann_freq_handle, i + '\t' + str(len(paper_conf[i]))

            paper_conf.clear()
    else:
        ann_conf = defaultdict(lambda:defaultdict(set))
    return ann_conf

def create_iterator(infile):
    # Returns an iterator object for an input uniprot-goa file along with a 
    # list of all field names contained in the uniprot-goa file

    infile_handle = open(infile, 'r')
    iter_handle = GOA.gafiterator(infile_handle)
    for ingen in iter_handle:
        if len(ingen) == 17:
            GAFFIELDS = GOA.GAF20FIELDS
            break
        else:
            GAFFIELDS = GOA.GAF10FIELDS
            break
    infile_handle = open(infile, 'r')
    iter_handle = GOA.gafiterator(infile_handle)
    return iter_handle, GAFFIELDS

def create_output(params, outfile, infile, work_dir, ontType):
    # Creates an output filename based on the output file prefix 
    # provided by the user
    if not outfile == '':
        ob = basename(params['outfile'])
    else:
        if ontType == '':
            ob = basename(infile + '.benchmark')
        else: 
            ob = basename(infile + '.benchmark' + '_' + ontType)
    index = 1
    while os.path.exists(work_dir + '/' + ob + '.' + str(index)):
        index = index + 1
    output_filename = ob + '.' + str(index)
    return output_filename

# The first step with using the argparse module is to create a parser object 
# that can then parse all the user inputs and convert them into python objects 
# based on the user input types provided

parser = argparse.ArgumentParser(prog='bm.py',description='Creates target sets and benchmark protein sets.')
parser.add_argument('-I1', '--input1', help='This opton is mandatory. Specifies path to first input file.')
parser.add_argument('-I2', '--input2', help='This option is mandatory. Specifies path to second input file.')
parser.add_argument('-O', '--output', default='', help='Provides user an option to specify an output filename prefix.')
parser.add_argument('-G','--organism',nargs='*', default=['all'],help='Provides user a choice ' + \
                        'to specify a set of organisms (example:Saccharomyces cerevisiae or 7227) ' + \
                        'separated by space. Default is all.')
parser.add_argument('-N','--ontology',nargs='*', default=['all'],help='Provides user a choice to ' + \
                        'specify a set of ontologies (F, P, C) separated by space. Default is all.')
parser.add_argument('-V','--evidence',nargs='*', default=['all'],help='Provides user a choice to ' + \
                        'specify a set of GO experimental evidence codes (example: IPI, IDA, EXP) ' + \
                        'separated by space.Default is all.')
parser.add_argument('-S', '--source',action='store' ,nargs='*',default=['all'],help='Provides user ' + \
                        'a choice to specify sources (example: UniProt, InterPro) separated by spaces. Default is all.')
parser.add_argument('-P', '--pubmed',default='F',help='Allows user to turn on the pubmed filter. ' + \
                        'If turned on, GO terms w/o any Pubmed references will not be considered ' + \
                        'part of the benchmark set. By default, it is turned off.')
parser.add_argument('-F', '--confidence',default='F',help='Allows user to turn on the annotation ' + \
                        'confidence filter. If turned on, GO terms assignments to proteins that are ' + \
                        'documented in few papers (4 or less by default) will not be considered part ' + \
                        'of the benchmark set.By default, it is turned off.')
parser.add_argument('-T', '--threshold',type=int, default=4,help='Allows users to specify a threshold ' + \
                        'for the minimum number of papers to be used for having a confident annotation. ' + \
                        'If not specified, defaults to a value of 4.')
parser.add_argument('-B', '--blacklist', nargs='*',default=[], help='This parameter can take in a list of ' + \
                        'pubmed ids and all GO terms and proteins annotated in them will be eliminated from ' + \
                        'the benchmark set.Default is an empty list.')

# Search for config file in the current directory. If not present, creates a 
# new config file

fname_ind = 0
for root,dirs,files in os.walk('.'):
    for fname in files:
        if fname == '.cafarc':
            fname_ind = 1
    if fname_ind == 0:
        print 'Configuration file not found'
        print 'Creating new configuration file ...'
        print '***************************************************************\n'
        Config.create()
    break
    
# Reads the config file and stores values in a dictionary
Config_handle = cp.ConfigParser()
Config_handle.read('.cafarc')
ConfigParam = defaultdict()

ConfigParam = {'workdir' : Config_handle.get('WORKDIR', 'DEFAULT_PATH'),
               'ftp_host' : Config_handle.get('FTP', 'HOSTNAME'),
               'ftp_curr_path' : Config_handle.get('FTP', 'CURRENT_FILE_PATH'),
               'ftp_old_path' : Config_handle.get('FTP', 'OLD_FILE_PATH'),
               'exp_eec' : Config_handle.get('DEFAULTS', 'EXP_EVIDENCE_CODES'),
               'ont_def' : Config_handle.get('DEFAULTS', 'ONTOLOGIES'),
               'tax_file' : Config_handle.get('DEFAULTS', 'TAXONOMY_FILENAME'),
               'uniprot_path' : Config_handle.get('SEQUENCE', 'BASE_URL'),
               'ftp_date' : Config_handle.get('REGEX', 'FTP_DATE'),
               'ftp_file_start' : Config_handle.get('REGEX', 'FTP_FILE_START')
               }

work_dir = ConfigParam['workdir']
work_dir = work_dir.rstrip('/')
uniprot_path = ConfigParam['uniprot_path'] .rstrip('/')

# In case the working directory is not found, the program will create one in this step
if not os.path.exists(work_dir):
    os.makedirs(work_dir)

# Parses the set of user given parameters and returns a dictionary of the same.
parsed_dict = ap.parse(parser, ConfigParam)
t1 = parsed_dict['t1']
t2 = parsed_dict['t2']
outfile_basename = basename(parsed_dict['outfile'])

# The conditional statement below checks if the user called the program in the default 
# mode or in Target Generation mode and proceeds further accordingly.

# Create input datasets based on whether the user chose CAFA or non-CAFA mode 
if parsed_dict['t1'] == parsed_dict['t2']:
    print 'Both input files are from the same time point. This will not create a valid benchmark set.'
    print 'Program quiting ...'
    sys.exit(1)

t1_input_file = work_dir + '/' + cd.parse(t1, ConfigParam) # Parse t1 file. 
t2_input_file = work_dir + '/' + cd.parse(t2, ConfigParam) # parse t2 file.

# Filename initialization
output_filename_LK_bpo = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'LK_bpo')
output_filename_LK_cco = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'LK_cco')
output_filename_LK_mfo = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'LK_mfo')
output_filename_NK_bpo = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'NK_bpo')
output_filename_NK_cco = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'NK_cco')
output_filename_NK_mfo = work_dir + '/' + create_output(
    parsed_dict, outfile_basename, t2_input_file, work_dir, 'NK_mfo')

t1_iea_name = t1_input_file + '.iea' 
      # This file will be used for recording entries in t1 file with 
      # non-experimental evidence codes
t1_exp_name = t1_input_file + '.exp'
      # This file will be used for recording entries in t1 file with 
      # experimental evidence codes
t2_exp_name = t2_input_file + '.exp'
      # This file will be used for recording entries in t2 file 
      # with experimental evidence codes

filename_LK_bpo = t2_exp_name + '.bpo_LK_bench.txt'
filename_LK_cco = t2_exp_name + '.cco_LK_bench.txt'
filename_LK_mfo = t2_exp_name + '.mfo_LK_bench.txt'
filename_NK_bpo = t2_exp_name + '.bpo_NK_bench.txt'
filename_NK_cco = t2_exp_name + '.cco_NK_bench.txt'
filename_NK_mfo = t2_exp_name + '.mfo_NK_bench.txt'

fc.check_gaf_format(t1_input_file) # Format checker module for input file t1
fc.check_gaf_format(t2_input_file) # Format checker module for input file t2

# Create paper-term frequency file
ann_conf = paper_term_freq(parsed_dict, t2_input_file)

# Creates an iterator object for t2 file
iter_handle, GAFFIELDS = create_iterator(t2_input_file)

# Creates a dictionary of taxon id-name mapping
tax_id_name_mapping = gc.parse_tax_file(ConfigParam['tax_file'])

# Filters t2 file for all proteins with experimentally annotated terms
# based on user defined parameters

print 'Parsing t2 file: ' + basename(t2_input_file) + ' ...'
t2_exp_handle = open(t2_exp_name, 'w')
for ingen in iter_handle: # iterate through t2 file
    retval = gc.record_has_forBenchmark(ingen,ann_conf, parsed_dict,
        tax_id_name_mapping, ConfigParam['exp_eec'], GAFFIELDS)
    if retval: # if retval is TRUE, write out the record to t2.exp file 
        GOA.writerec(ingen,t2_exp_handle,GAFFIELDS) # creates t2.exp file
t2_exp_handle.close()
    
# If t2.exp file is empty, it would end up producing an empty benchmark file. 
# To prevent this, the program will quit here with a message.
if os.stat(t2_exp_name).st_size == 0:
    print 'Your benchmark set will be empty with the parameters provided. Quiting ...'
    sys.exit(1)

# Filters t1 file and creates benchmark
iter_handle, GAFFIELDS = create_iterator(t1_input_file)
print 'Parsing t1 file: ' + basename(t1_input_file) + ' ...' 
gc.t1_filter(
    iter_handle, t1_iea_name, t1_exp_name, t2_exp_name, GAFFIELDS, ConfigParam['exp_eec'])
           # This methods creats the t1.iea and t1.exp files

print 'Creating benchmark set ...'
fb.filter_benchmarks(t1_iea_name, t1_exp_name, t2_exp_name) 
          # Filters benchmarks and create SIX benchmark files

#sd.down(output_filename, uniprot_path, unique_proteins)

# Checks the size of the benchmark files created in cb.filter_benchmark method.
# If any file is found to be empty, the program will print a message saying so. 
# Otherwise, it will remove the redundant entries in that file and create a 
# final benchmark file with unique benchmark entries. It will do so for each of
# the SIX benchmark files.
if os.stat(filename_LK_bpo).st_size == 0:
    print 'Your benchmark set for Biological Process Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_LK_bpo +  " | sort | uniq " + " > " + output_filename_LK_bpo
    subprocess.call(cmd, shell=True)

if os.stat(filename_LK_cco).st_size == 0:
    print 'Your benchmark set for Cellular Component Process Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_LK_cco +  " | sort | uniq " + " > " + output_filename_LK_cco
    subprocess.call(cmd, shell=True)

if os.stat(filename_LK_mfo).st_size == 0:
    print 'Your benchmark set for Molecular Function Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_LK_mfo +  " | sort | uniq " + " > " + output_filename_LK_mfo
    subprocess.call(cmd, shell=True)

if os.stat(filename_NK_bpo).st_size == 0:
    print 'Your no-knowledge benchmark set for Biological Process Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_NK_bpo +  " | sort | uniq " + " > " + output_filename_NK_bpo
    subprocess.call(cmd, shell=True)

if os.stat(filename_NK_cco).st_size == 0:
    print 'Your no-knowledge benchmark set for Cellular Component Process Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_NK_cco +  " | sort | uniq " + " > " + output_filename_NK_cco
    subprocess.call(cmd, shell=True)

if os.stat(filename_NK_mfo).st_size == 0:
    print 'Your no-knowledge benchmark set for Molecular Function Ontology is empty.'
else:
    cmd = "awk -F \'\t\' '{print;}' " + filename_NK_mfo +  " | sort | uniq " + " > " + output_filename_NK_mfo
    subprocess.call(cmd, shell=True)

# Cleans the working directory by removing files created in the 
# intermediate steps. These files include files for iea and exp from 
# t1 and t2 input files and filter benchmark files
print 'Cleaning working directory ...'
# Removing the temporary benchmark files created by fb.filter_benchmark() method
os.remove(filename_LK_bpo)
os.remove(filename_LK_cco)
os.remove(filename_LK_mfo)
os.remove(filename_NK_bpo)
os.remove(filename_NK_cco)
os.remove(filename_NK_mfo)
os.remove(t1_input_file + '.iea')
if os.path.exists(t1_input_file + '.exp'):
    os.remove(t1_input_file + '.exp')
os.remove(t2_exp_name)
os.remove(t2_input_file + '_with_annotations_per_paper.txt')
#Deletes all empty files from the working directory and its subdirectories
for root, dirs, files in os.walk(work_dir):
    for fname in files:
        if os.path.getsize(root + '/' + fname) == 0:
            os.remove(root + '/' + fname)
    break
print bcolors.OKGREEN + 'The following benchmark files are created:' + bcolors.ENDC 
if os.path.exists(output_filename_LK_bpo):
    print basename(output_filename_LK_bpo)
if os.path.exists(output_filename_LK_cco):
    print basename(output_filename_LK_cco)
if os.path.exists(output_filename_LK_mfo):
    print basename(output_filename_LK_mfo)
if os.path.exists(output_filename_NK_bpo):
    print basename(output_filename_NK_bpo)
if os.path.exists(output_filename_NK_cco):
    print basename(output_filename_NK_cco)
if os.path.exists(output_filename_NK_mfo):
    print basename(output_filename_NK_mfo)
print bcolors.OKGREEN + 'Thank you for using Benchmark Creation Tool' + bcolors.ENDC 
